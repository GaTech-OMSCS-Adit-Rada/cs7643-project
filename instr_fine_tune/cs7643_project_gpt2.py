# -*- coding: utf-8 -*-
"""cs7643_project_gpt2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oZ1-UkgkGoe8sysXLBXHpa35mocjdQr4

# CS7643: GPT2 From Scratch & Fine Tuning

# 0. Setup

### [Optional] Set up Google Drive Connection
"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext autoreload
# %autoreload 2

from google.colab import drive
drive.mount('/content/drive')

import os

# TODO: Fill in the Google Drive path where you uploaded assignment1
# Example: If you create a Fall2023 folder and put all the files under A1 folder, then 'Fall2023/A1'
GOOGLE_DRIVE_PATH_POST_MYDRIVE = 'cs7643/project'
GOOGLE_DRIVE_PATH = os.path.join('/content', 'drive', 'MyDrive', GOOGLE_DRIVE_PATH_POST_MYDRIVE)
print(os.listdir(GOOGLE_DRIVE_PATH))

import sys
import numpy as np
import math
sys.path.append(GOOGLE_DRIVE_PATH)

sys.path

# if running locally set GOOGLE PATH
import sys
if 'google.colab' in sys.modules:
  print(f'Running in google colab. Our path is `{GOOGLE_DRIVE_PATH}`')
else:
  GOOGLE_DRIVE_PATH = '.'
  print('Running locally.')

"""### Import Modules"""

!pip install tiktoken

from importlib.metadata import version
import tiktoken
print("tiktoken version:", version("tiktoken"))

import torch
print("torch version:", version("torch"))

device = 'mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available() else 'cpu')
print("Using device = " + device)
if device == 'cpu':
    print("WARNING: Using CPU will cause slower train times")

from torch.utils.data import Dataset, DataLoader

from torch import nn

import pandas as pd

import urllib
from pathlib import Path
import time
from tqdm import tqdm

"""# 1. Implement a GPT-2 Model"""

from model.gpt import GPTModel, text_to_token_ids, token_ids_to_text, generate_text_simple, generate, print_model_stats, TransformerBlock
from model.load_model import load_weights
from model.lora_gpt import LoRALayer, LinearWithLoRA, replace_linear_with_lora, replace_linear_with_lora_last_n

"""## 1.0 Utils"""

from transformers import GPT2Model

# Available Models Names
model_names = {
    "gpt2-small (124M)": "openai-community/gpt2",
    "gpt2-medium (355M)": "openai-community/gpt2-medium",
    "gpt2-large (774M)": "openai-community/gpt2-large",
    "gpt2-xl (1558M)": "openai-community/gpt2-xl"
}

BASE_CONFIG = {
    "vocab_size": 50257,     # Vocabulary size
    "context_length": 1024,  # Context length
    "drop_rate": 0.0,        # Dropout rate
    "qkv_bias": True         # Query-key-value bias
}

model_configs = {
    "gpt2-small (124M)": {"emb_dim": 768, "n_layers": 12, "n_heads": 12},
    "gpt2-medium (355M)": {"emb_dim": 1024, "n_layers": 24, "n_heads": 16},
    "gpt2-large (774M)": {"emb_dim": 1280, "n_layers": 36, "n_heads": 20},
    "gpt2-xl (1558M)": {"emb_dim": 1600, "n_layers": 48, "n_heads": 25},
}

def get_raw_gpt(model_name):
    if model_name not in model_configs:
        raise ValueError(f"Unknown model name: {model_name}")

    base_config_copy = BASE_CONFIG.copy()
    base_config_copy.update(model_configs[model_name])
    return GPTModel(base_config_copy)

def get_pretrained_gpt_model(model_name, verbose=True):
    if model_name not in model_configs:
        raise ValueError(f"Unknown model name: {model_name}")

    base_config_copy = BASE_CONFIG.copy()
    base_config_copy.update(model_configs[model_name])
    gpt_model = GPTModel(base_config_copy)

    hf_pretrained_gpt = GPT2Model.from_pretrained(model_names[model_name], cache_dir="checkpoints")
    load_weights(gpt_model, hf_pretrained_gpt, base_config_copy)

    if verbose:
        print_model_stats(gpt_model, model_name)

    return gpt_model

def convert_to_lora_model(model: GPTModel, rank: int, alpha: int, last_n_trf_blocks=None) -> GPTModel:
    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Total trainable parameters before: {total_params:,}")

    for param in model.parameters():
        param.requires_grad = False

    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Total trainable parameters after: {total_params:,}")

    if last_n_trf_blocks is not None:
        replace_linear_with_lora_last_n(model, n=last_n_trf_blocks, rank=rank, alpha=alpha)
    else:
        replace_linear_with_lora(model, rank=rank, alpha=alpha)

    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Total trainable LoRA parameters: {total_params:,}")
    model.to(device)
    return model

"""## 1.1 Sanity Check Creating GPT Model"""

GPT_CONFIG_124M = {
    "vocab_size": 50257,     # Vocabulary size
    "context_length": 1024,  # Context length
    "emb_dim": 768,          # Embedding dimension
    "n_heads": 12,           # Number of attention heads
    "n_layers": 12,          # Number of layers
    "drop_rate": 0.1,        # Dropout rate
    "qkv_bias": False        # Query-Key-Value bias
}

torch.manual_seed(123)
test_raw_gpt = GPTModel(GPT_CONFIG_124M)
test_raw_gpt.eval()  # disable dropout

start_context = "Hello, I am"

tokenizer = tiktoken.get_encoding("gpt2")
encoded_tensor = text_to_token_ids(start_context, tokenizer)

print(f"\n{50*'='}\n{22*' '}IN\n{50*'='}")
print("\nInput text:", start_context)
print("Encoded input text:", encoded_tensor)
print("encoded_tensor.shape:", encoded_tensor.shape)

out_token_ids = generate_text_simple(
    model=test_raw_gpt,
    token_ids=encoded_tensor,
    max_new_tokens=10,
    context_size=GPT_CONFIG_124M["context_length"]
)
decoded_text = token_ids_to_text(out_token_ids, tokenizer)

print(f"\n\n{50*'='}\n{22*' '}OUT\n{50*'='}")
print("\nOutput:", out_token_ids)
print("Output length:", len(out_token_ids[0]))
print("Output text:", decoded_text)

total_params = sum(p.numel() for p in test_raw_gpt.parameters())
print(f"Total Parameters: {total_params:,}")

total_params_gpt2 =  total_params - sum(p.numel() for p in test_raw_gpt.out_head.parameters())
print(f"Number of trainable parameters considering weight tying: {total_params_gpt2:,}")

print_model_stats(test_raw_gpt, "GPT-124M")

"""## 1.2 Sanity Check Loading Pre-trained Weights"""

# CHOOSE_MODEL = "gpt2-medium (355M)"
CHOOSE_MODEL = "gpt2-large (774M)"
test_pretrained_gpt = get_pretrained_gpt_model(CHOOSE_MODEL, verbose=False)

torch.manual_seed(123)

tokenizer = tiktoken.get_encoding("gpt2")

token_ids = generate_text_simple(
    model=test_pretrained_gpt.to(device),
    # token_ids=text_to_token_ids("Every effort moves", tokenizer).to(device),
    token_ids=text_to_token_ids("The state capital of New Jersey is Newark. The state capital of California is", tokenizer).to(device),
    max_new_tokens=30,
    context_size=BASE_CONFIG["context_length"],
)

print("Output text:\n", token_ids_to_text(token_ids, tokenizer))

"""## 1.3 Sanity check LoRA model"""

test_lora_gpt = get_pretrained_gpt_model("gpt2-small (124M)", verbose=False)

total_params = sum(p.numel() for p in test_lora_gpt.parameters() if p.requires_grad)
print(f"Total trainable parameters before: {total_params:,}")

for param in test_lora_gpt.parameters():
    param.requires_grad = False

total_params = sum(p.numel() for p in test_lora_gpt.parameters() if p.requires_grad)
print(f"Total trainable parameters after: {total_params:,}")

# replace_linear_with_lora(test_lora_gpt, rank=16, alpha=16)
replace_linear_with_lora_last_n(test_lora_gpt, n=2, rank=16, alpha=16)

total_params = sum(p.numel() for p in test_lora_gpt.parameters() if p.requires_grad)
print(f"Total trainable LoRA parameters: {total_params:,}")

torch.manual_seed(123)

tokenizer = tiktoken.get_encoding("gpt2")

token_ids = generate_text_simple(
    model=test_lora_gpt.to(device),
    token_ids=text_to_token_ids("Every effort moves", tokenizer).to(device),
    max_new_tokens=30,
    context_size=BASE_CONFIG["context_length"],
)

print("Output text:\n", token_ids_to_text(token_ids, tokenizer))



"""# 3. The Verdict Dataset Loaders"""

class GPTDatasetV1(Dataset):
    def __init__(self, txt, tokenizer, max_length, stride):
        self.input_ids = []
        self.target_ids = []

        # Tokenize the entire text
        token_ids = tokenizer.encode(txt, allowed_special={"<|endoftext|>"})

        # Use a sliding window to chunk the book into overlapping sequences of max_length
        for i in range(0, len(token_ids) - max_length, stride):
            input_chunk = token_ids[i:i + max_length]
            target_chunk = token_ids[i + 1: i + max_length + 1]
            self.input_ids.append(torch.tensor(input_chunk))
            self.target_ids.append(torch.tensor(target_chunk))

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx], self.target_ids[idx]


def create_dataloader_v1(txt, batch_size=4, max_length=256,
                         stride=128, shuffle=True, drop_last=True, num_workers=0):
    # Initialize the tokenizer
    tokenizer = tiktoken.get_encoding("gpt2")

    # Create dataset
    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)

    # Create dataloader
    dataloader = DataLoader(
        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)

    return dataloader

"""### Test on simple "The Verdict" data"""

verdict_file_path = GOOGLE_DRIVE_PATH + '/the-verdict.txt'
# verdict_file_path = 'the-verdict.txt'       # If loaded directly to Google Colab Runtime

with open(verdict_file_path, "r", encoding="utf-8") as f:
    raw_text = f.read()

dataloader = create_dataloader_v1(
    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)
data_iter = iter(dataloader)
first_batch = next(data_iter)
print(first_batch)

raw_text

"""# 4. Training Loop"""

from instr_fine_tune.train import calc_batch_loss, calc_loader_loss, evaluate_model, generate_and_print_sample, TrainingConfig, TrainingResults, train_model_simple, train_model
from instr_fine_tune.train_plots import plot_losses, plot_perplexity, plot_and_save_learning_rate

def save_training_results(results: TrainingResults, filename: str):
    results_dict = {
        "train_losses": results.train_losses,
        "val_losses": results.val_losses,
        "track_tokens_seen": results.track_tokens_seen,
        "train_perplexity": results.train_perplexity,
        "val_perplexity": results.val_perplexity
    }
    if results.track_lrs is not None:
        results_dict["track_lrs"] = results.track_lrs

    with open(filename, "w") as f:
        json.dump(results_dict, f)
        print(f"Training results saved to {filename}")

"""## 4.2 Simple Train loop"""

GPT_CONFIG_124M = {
    "vocab_size": 50257,   # Vocabulary size
    "context_length": 256, # Shortened context length (orig: 1024)
    "emb_dim": 768,        # Embedding dimension
    "n_heads": 12,         # Number of attention heads
    "n_layers": 12,        # Number of layers
    "drop_rate": 0.1,      # Dropout rate
    "qkv_bias": False      # Query-key-value bias
}

torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
model.eval();  # Disable dropout during inference

with open(verdict_file_path, "r", encoding="utf-8") as file:
    text_data = file.read()

print(text_data[:99])

total_characters = len(text_data)
total_tokens = len(tokenizer.encode(text_data))

print("Characters:", total_characters)
print("Tokens:", total_tokens)

# Train/validation ratio
train_ratio = 0.90
split_idx = int(train_ratio * len(text_data))
train_data = text_data[:split_idx]
val_data = text_data[split_idx:]


torch.manual_seed(123)

train_loader = create_dataloader_v1(
    train_data,
    batch_size=2,
    max_length=GPT_CONFIG_124M["context_length"],
    stride=GPT_CONFIG_124M["context_length"],
    drop_last=True,
    shuffle=True,
    num_workers=0
)

val_loader = create_dataloader_v1(
    val_data,
    batch_size=2,
    max_length=GPT_CONFIG_124M["context_length"],
    stride=GPT_CONFIG_124M["context_length"],
    drop_last=False,
    shuffle=False,
    num_workers=0
)

train_tokens = 0
for input_batch, target_batch in train_loader:
    train_tokens += input_batch.numel()

val_tokens = 0
for input_batch, target_batch in val_loader:
    val_tokens += input_batch.numel()

print("Training tokens:", train_tokens)
print("Validation tokens:", val_tokens)
print("All tokens:", train_tokens + val_tokens)

print("Train loader:")
for x, y in train_loader:
    print(x.shape, y.shape)

print("\nValidation loader:")
for x, y in val_loader:
    print(x.shape, y.shape)

"""### 4.2.1 Sanity check the simple training loop"""

torch.manual_seed(123)
GPT_CONFIG_124M = {
    "vocab_size": 50257,   # Vocabulary size
    "context_length": 256, # Shortened context length (orig: 1024)
    "emb_dim": 768,        # Embedding dimension
    "n_heads": 12,         # Number of attention heads
    "n_layers": 12,        # Number of layers
    "drop_rate": 0.1,      # Dropout rate
    "qkv_bias": False      # Query-key-value bias
}

model = GPTModel(GPT_CONFIG_124M)
model.to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)

num_epochs = 10
training_config = TrainingConfig(
    model=model,
    train_loader=train_loader,
    val_loader=val_loader,
    optimizer=optimizer,
    device=device,
    num_epochs=num_epochs,
    eval_freq=5,
    eval_iter=5,
    start_context="Every effort moves you",
    tokenizer=tokenizer,
)

sanity_check_training_results = train_model_simple(training_config)

epochs_tensor = torch.linspace(0, num_epochs, len(sanity_check_training_results.train_losses))
plot_losses(epochs_tensor,
            sanity_check_training_results.track_tokens_seen,
            sanity_check_training_results.train_losses,
            sanity_check_training_results.val_losses)

"""## 4.3 Advanced Train Loop

This training loop adds the following to the simple one:

1) Learning rate warmup

2) Cosine decay (after pear l.r has been reached)

3) Gradient clipping

### 4.3.1 Sanity check the advanced training loop

Typically, the number of warmup steps is between 0.1% to 20% of the total number of steps
"""

num_epochs = 15
total_steps = len(train_loader) * num_epochs
warmup_steps = int(0.2 * total_steps) # 20% warmup
print(warmup_steps)

torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
model.to(device)

peak_lr = 0.001
optimizer = torch.optim.AdamW(model.parameters(), lr=peak_lr, weight_decay=0.1)
tokenizer = tiktoken.get_encoding("gpt2")

num_epochs = 15
training_config = TrainingConfig(
    model=model,
    train_loader=train_loader,
    val_loader=val_loader,
    optimizer=optimizer,
    device=device,
    num_epochs=num_epochs,
    eval_freq=5,
    eval_iter=1,
    start_context="Every effort moves you",
    tokenizer=tokenizer,
    warmup_steps=warmup_steps,
    initial_lr=1e-5,
    min_lr=1e-5,
)

sanity_check_advanced_training_results = train_model(training_config)

epochs_tensor = torch.linspace(1, num_epochs, len(sanity_check_advanced_training_results.train_losses))

plot_losses(epochs_tensor,
            sanity_check_advanced_training_results.track_tokens_seen,
            sanity_check_advanced_training_results.train_losses,
            sanity_check_advanced_training_results.val_losses)
plt.tight_layout(); plt.savefig("3.pdf")
plt.show()

plt.figure(figsize=(7, 5))
plt.plot(range(len(sanity_check_advanced_training_results.track_lrs)), sanity_check_advanced_training_results.track_lrs)
plt.ylabel("Learning rate")
plt.xlabel("Steps")
plt.grid()
plt.show()



"""# 5. Mini-Alpaca Instruction Fine Tuning

## 5.0 Utils
"""

def print_model_losses(model, train_loader, val_loader, device):
    with torch.no_grad():
        train_loss = calc_loader_loss(train_loader, model, device, num_batches=5)
        val_loss = calc_loader_loss(val_loader, model, device, num_batches=5)

    print("Training loss:", train_loss)
    print("Validation loss:", val_loss)
    print("Training perplexity:", torch.exp(torch.tensor(train_loss)).item())
    print("Validation perplexity:", torch.exp(torch.tensor(val_loss)).item())

import json

def download_and_load_json_file(file_path, url):
    if not os.path.exists(file_path):
        with urllib.request.urlopen(url) as response:
            text_data = response.read().decode("utf-8")
        with open(file_path, "w", encoding="utf-8") as file:
            file.write(text_data)

    with open(file_path, "r", encoding="utf-8") as file:
        data = json.load(file)

    return data

def format_input(entry):
    instruction_text = (
        f"Below is an instruction that describes a task. "
        f"Write a response that appropriately completes the request."
        f"\n\n### Instruction:\n{entry['instruction']}"
    )

    input_text = f"\n\n### Input:\n{entry['input']}" if entry["input"] else ""

    return instruction_text + input_text

def format_response(entry):
    return f"\n\n### Response:\n{entry['output']}"

def load_json_data(filepath):
    with open(filepath, 'r') as f:
        data = json.load(f)
    return data

def generate_model_response(model, test_data, file_path=None):
    model.eval()
    with torch.no_grad():
        for i, entry in tqdm(enumerate(test_data), total=len(test_data)):
            instruction_text = format_input(entry)
            token_ids = generate(
                model=model.to(device),
                token_ids=text_to_token_ids(instruction_text, tokenizer).to(device),
                max_new_tokens=512,
                context_size=BASE_CONFIG["context_length"],
                eos_id=50256,
            )
            generated_text = token_ids_to_text(token_ids, tokenizer)

            response_text = generated_text[len(instruction_text):].replace("### Response:", "").strip()
            entry["generated_text"] = response_text

    # Save the data
    if file_path:
        with open(file_path, 'w') as f:
            json.dump(test_data, f, indent=4)

    return test_data

def print_model_losses(model, train_loader, val_loader, device):
    with torch.no_grad():
        train_loss = calc_loader_loss(train_loader, model, device, num_batches=5)
        val_loss = calc_loader_loss(val_loader, model, device, num_batches=5)

    print("Training loss:", train_loss)
    print("Validation loss:", val_loss)
    print("Training perplexity:", torch.exp(torch.tensor(train_loss)).item())
    print("Validation perplexity:", torch.exp(torch.tensor(val_loss)).item())
    print()

"""## 5.2 Organizing data into training batches"""

class InstructionDataset(Dataset):
    def __init__(self, data, tokenizer):
        self.data = data

        # Pre-tokenize texts
        self.encoded_texts = []
        for entry in data:
            instruction_plus_input = format_input(entry)
            response_text = format_response(entry)
            full_text = instruction_plus_input + response_text
            self.encoded_texts.append(
                tokenizer.encode(full_text)
            )

    def __getitem__(self, index):
        return self.encoded_texts[index]

    def __len__(self):
        return len(self.data)

def custom_collate_fn(
    batch,
    pad_token_id=50256,
    ignore_index=-100,
    allowed_max_length=None,
    device="cpu"
):
    # Find the longest sequence in the batch
    batch_max_length = max(len(item)+1 for item in batch)

    # Pad and prepare inputs and targets
    inputs_lst, targets_lst = [], []

    for item in batch:          # item is just list of token ids e.g. [0, 1, 2, 3]
        new_item = item.copy()
        # Add an <|endoftext|> token
        new_item += [pad_token_id]
        # Pad sequences to max_length
        padded = (
            new_item + [pad_token_id] *
            (batch_max_length - len(new_item))
        )
        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs
        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets

        # New: Replace all but the first padding tokens in targets by ignore_index
        mask = targets == pad_token_id
        indices = torch.nonzero(mask).squeeze()
        if indices.numel() > 1:
            targets[indices[1:]] = ignore_index

        # New: Optionally truncate to maximum sequence length
        if allowed_max_length is not None:
            inputs = inputs[:allowed_max_length]
            targets = targets[:allowed_max_length]

        inputs_lst.append(inputs)
        targets_lst.append(targets)

    # Convert list of inputs and targets to tensors and transfer to target device
    inputs_tensor = torch.stack(inputs_lst).to(device)
    targets_tensor = torch.stack(targets_lst).to(device)

    return inputs_tensor, targets_tensor

from functools import partial

customized_collate_fn = partial(
    custom_collate_fn,
    device=device,
    allowed_max_length=1024
)

"""## 5.1 Prepare Dataset"""

file_path = "instruction-data.json"
url = (
    "https://raw.githubusercontent.com/rasbt/LLMs-from-scratch"
    "/main/ch07/01_main-chapter-code/instruction-data.json"
)

data = download_and_load_json_file(file_path, url)
print("Number of entries:", len(data))

print("Example entry:\n", data[50])

def save_first_n_entries(data, n, filename):
    """Saves the first n entries of the data to a JSON file."""
    try:
        with open(filename, 'w') as f:
            json.dump(data[:n], f, indent=4)  # Use indent for pretty printing
        print(f"Successfully saved the first {n} entries to {filename}")
    except Exception as e:
        print(f"An error occurred: {e}")

save_first_n_entries(
    data, 30,
    GOOGLE_DRIVE_PATH + "/instr_fine_tune" + "/data/alpaca_sample_generation_data_30.json")

print(format_input(data[50]) + format_response(data[50]))

train_portion = int(len(data) * 0.85)  # 85% for training
test_portion = int(len(data) * 0.1)    # 10% for testing
val_portion = len(data) - train_portion - test_portion  # Remaining 5% for validation

train_data = data[:train_portion]
test_data = data[train_portion:train_portion + test_portion]
val_data = data[train_portion + test_portion:]

print("Training set length:", len(train_data))
print("Validation set length:", len(val_data))
print("Test set length:", len(test_data))

"""## 5.3 Creating data loaders for an instruction dataset"""

tokenizer = tiktoken.get_encoding("gpt2")

num_workers = 0
batch_size = 8

torch.manual_seed(123)

train_dataset = InstructionDataset(train_data, tokenizer)
train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    collate_fn=customized_collate_fn,
    shuffle=True,
    drop_last=True,
    num_workers=num_workers
)

val_dataset = InstructionDataset(val_data, tokenizer)
val_loader = DataLoader(
    val_dataset,
    batch_size=batch_size,
    collate_fn=customized_collate_fn,
    shuffle=False,
    drop_last=False,
    num_workers=num_workers
)

test_dataset = InstructionDataset(test_data, tokenizer)
test_loader = DataLoader(
    test_dataset,
    batch_size=batch_size,
    collate_fn=customized_collate_fn,
    shuffle=False,
    drop_last=False,
    num_workers=num_workers
)

print("Train loader:")
i = 0
for inputs, targets in train_loader:
    print(inputs.shape, targets.shape)
    if i == 3: break
    i += 1

"""## 5.4 Loading a pretrained LLM"""

CHOOSE_MODEL = "gpt2-medium (355M)"
gpt_m = get_pretrained_gpt_model(CHOOSE_MODEL)

input_text = format_input(val_data[0])
print(input_text)

torch.manual_seed(123)

tokenizer = tiktoken.get_encoding("gpt2")

token_ids = generate_text_simple(
    model=gpt_m.to(device),
    token_ids=text_to_token_ids(input_text, tokenizer).to(device),
    max_new_tokens=35,
    context_size=BASE_CONFIG["context_length"],
)

generated_text = token_ids_to_text(token_ids, tokenizer)
print("Output text:\n", generated_text)

response_text = (
    generated_text[len(input_text):]
    .replace("### Response:", "")
    .strip()
)
print(response_text)

"""## 5.5 Finetuning the LLM on instruction data

Initial Loss
"""

torch.manual_seed(123)

with torch.no_grad():
    train_loss = calc_loader_loss(train_loader, gpt_m, device, num_batches=5)
    val_loss = calc_loader_loss(val_loader, gpt_m, device, num_batches=5)

print("Training loss:", train_loss)
print("Validation loss:", val_loss)
print("Training perplexity:", torch.exp(torch.tensor(train_loss)).item())
print("Validation perplexity:", torch.exp(torch.tensor(val_loss)).item())

import time

start_time = time.time()

torch.manual_seed(123)

optimizer = torch.optim.AdamW(gpt_m.parameters(), lr=0.00005, weight_decay=0.1)
num_epochs = 2

ift_mini_alpaca_training_config = TrainingConfig(
    model=gpt_m,
    train_loader=train_loader,
    val_loader=val_loader,
    optimizer=optimizer,
    device=device,
    num_epochs=num_epochs,
    eval_freq=5,
    eval_iter=5,
    start_context=format_input(val_data[0]),
    tokenizer=tokenizer,
)

ift_mini_alpaca_training_results = train_model_simple(ift_mini_alpaca_training_config)

end_time = time.time()
execution_time_minutes = (end_time - start_time) / 60
print(f"Training completed in {execution_time_minutes:.2f} minutes.")

epochs_tensor = torch.linspace(
    0, num_epochs, len(ift_mini_alpaca_training_results.train_losses))

plot_losses(epochs_tensor,
            ift_mini_alpaca_training_results.track_tokens_seen,
            ift_mini_alpaca_training_results.train_losses,
            ift_mini_alpaca_training_results.val_losses)

token_ids = generate(
    model=gpt_m.to(device),
    token_ids=text_to_token_ids(input_text, tokenizer).to(device),
    max_new_tokens=35,
    context_size=BASE_CONFIG["context_length"],
    eos_id=50256,
)

generated_text = token_ids_to_text(token_ids, tokenizer)
print("Output text:\n", generated_text)

"""# 6. Alpaca Instruction Fine Tuning

## 6.0 [Optional] Download the ALpaca Dataset
"""

alpaca_data_url = "https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json"
file_path = GOOGLE_DRIVE_PATH + "/instr_fine_tune" + "/data/alpaca_data.json"

alpaca_data = download_and_load_json_file(file_path, alpaca_data_url)
print("Number of entries:", len(alpaca_data))

"""We will select only 12K entries of the 52K for compute reasons.

We will only keep entires whose output is less than 512 characters in length
"""

def filter_alpaca_data_by_len(alpaca_data, target_size=12000, max_output_len=512, filepath=None):
    filtered_alpaca_data = []

    for entry in alpaca_data:
            if len(entry['output']) < max_output_len:
                filtered_alpaca_data.append(entry)
            if len(filtered_alpaca_data) >= target_size:
                break

    if filepath:
        with open(filepath, 'w') as f:
            json.dump(filtered_alpaca_data, f, indent=4)

    return filtered_alpaca_data

alpaca_data_15k = filter_alpaca_data_by_len(
    alpaca_data, target_size=15000, max_output_len=512,
    filepath=GOOGLE_DRIVE_PATH + "/instr_fine_tune" + "/data/alpaca_data_15k.json")

print("Number of entries:", len(alpaca_data_15k))

"""## Gemini Eval"""

from google import genai
from google.genai import types
from google.colab import userdata


def gemini_generate(prompt):
    client = genai.Client(
        api_key=userdata.get('GOOGLE_API_KEY')
    )

    # model = "gemini-2.5-pro-exp-03-25"
    model = "gemini-2.0-flash-thinking-exp-01-21"
    contents = [
        types.Content(
            role="user",
            parts=[
                types.Part.from_text(text=prompt),
            ],
        ),
    ]
    generate_content_config = types.GenerateContentConfig(
        response_mime_type="text/plain",
    )

    for tries in range(3):
        try:
            response = client.models.generate_content(
                model=model,
                contents=contents,
                config=generate_content_config,
            )
            return response.text
        except Exception as e:
            print(f"An error occurred: {e}")
            time.sleep(30)
            continue

gemini_generate("Hello, how are you?")

def generate_model_scores(json_data, filepath=None):
    scores = []
    for entry in tqdm(json_data, desc="Scoring entries"):
        prompt = (
            f"Given the input `{format_input(entry)}` "
            f"and correct output `{entry['output']}`, "
            f"score the model response `{entry['generated_text']}`"
            f" on a scale from 0 to 100, where 100 is the best score. "
            f"Respond with the integer number only."
        )
        score = gemini_generate(prompt)
        try:
            scores.append(int(score))
            print(f"Score: {score}")
            entry["score"] = score
        except ValueError:
            print(f"Could not convert score: {score}")
            continue
        except Exception as e:
            print(f"An error occurred: {e}")
            time.sleep(5)
            continue

    if filepath:
        with open(filepath, 'w') as f:
            json.dump(json_data, f, indent=4)

    print(f"Mean score: {np.mean(scores)}")

    return scores

"""## 6.1 Prepare the Dataset"""

alpaca_data_15k_filepath = GOOGLE_DRIVE_PATH + "/instr_fine_tune" + "/data/alpaca_data_15k.json"
alpaca_data_15k = load_json_data(alpaca_data_15k_filepath)

alpaca_sample_generation_data_filepath = GOOGLE_DRIVE_PATH + "/instr_fine_tune" + "/data/alpaca_sample_generation_data_30.json"
alpaca_sample_generation_data = load_json_data(alpaca_sample_generation_data_filepath)

num_entries = len(alpaca_data_15k)
print("Number of entries:", num_entries)

train_portion = int(num_entries * 0.80)  # 85% for training
test_portion = int(num_entries * 0.10)    # 10% for testing
val_portion = num_entries - train_portion - test_portion  # Remaining 10% for validation

alpaca_train_data = alpaca_data_15k[:train_portion]
alpaca_test_data = alpaca_data_15k[train_portion:train_portion + test_portion]
alpaca_val_data = alpaca_data_15k[train_portion + test_portion:]

print("Training set length:", len(alpaca_train_data))
print("Validation set length:", len(alpaca_val_data))
print("Test set length:", len(alpaca_test_data))

subset_alpaca_test_data = alpaca_sample_generation_data + alpaca_test_data[:20]
print("Subset of test data length:", len(subset_alpaca_test_data))

"""## 6.2 Create data loaders"""

num_workers = 0
batch_size = 2

torch.manual_seed(123)
tokenizer = tiktoken.get_encoding("gpt2")

alpaca_train_dataset = InstructionDataset(alpaca_train_data, tokenizer)
alpaca_train_loader = DataLoader(
    alpaca_train_dataset,
    batch_size=batch_size,
    collate_fn=customized_collate_fn,
    shuffle=True,
    drop_last=True,
    num_workers=num_workers,
)

alpaca_val_dataset = InstructionDataset(alpaca_val_data, tokenizer)
alpaca_val_loader = DataLoader(
    alpaca_val_dataset,
    batch_size=batch_size,
    collate_fn=customized_collate_fn,
    shuffle=False,
    drop_last=False,
    num_workers=num_workers,
)

alpaca_test_dataset = InstructionDataset(alpaca_test_data, tokenizer)
alpaca_test_loader = DataLoader(
    alpaca_test_dataset,
    batch_size=batch_size,
    collate_fn=customized_collate_fn,
    shuffle=False,
    drop_last=False,
    num_workers=num_workers,
)

print("Train loader:")
i = 0
for inputs, targets in alpaca_train_loader:
    if i <= 3:
        print(inputs.shape, targets.shape)
    i += 1
print(f"There are {i} batches in the train loader")

"""## 6.3 Load Pretrained GPT2"""

CHOOSE_MODEL = "gpt2-xl (1558M)"
# gpt2-large (774M)
pretrained_gpt_m = get_pretrained_gpt_model(CHOOSE_MODEL)

generated_data_file_path = GOOGLE_DRIVE_PATH + "/instr_fine_tune" + "/data/runs/pretrained_gpt_xl_generated_data.json"
generated_data = generate_model_response(
    model=pretrained_gpt_m,
    test_data=subset_alpaca_test_data,
    file_path=generated_data_file_path
)
generated_data_scores = generate_model_scores(generated_data, generated_data_file_path)

sample_text = format_input(alpaca_val_data[7])
print(sample_text)

torch.manual_seed(123)

token_ids = generate_text_simple(
    model=pretrained_gpt_m.to(device),
    token_ids=text_to_token_ids(sample_text, tokenizer).to(device),
    max_new_tokens=35,
    context_size=BASE_CONFIG["context_length"],
)

generated_text = token_ids_to_text(token_ids, tokenizer)
print("Output text:\n", generated_text)

response_text = (
    generated_text[len(sample_text):]
    .replace("### Response:", "")
    .strip()
)
print(response_text)

"""## 6.4 Finetune on Alpaca 12k"""

def finetune_model(run_name):
    # 1) Initialize a Pre-trained GPT model
    # CHOOSE_MODEL = "gpt2-medium (355M)"
    CHOOSE_MODEL = "gpt2-large (774M)"
    pretrained_gpt_m = get_pretrained_gpt_model(CHOOSE_MODEL)
    pretrained_gpt_m.to(device)

    pretrained_gpt_m = convert_to_lora_model(pretrained_gpt_m, rank=16, alpha=16, last_n_trf_blocks=None)
    print_model_losses(pretrained_gpt_m, alpaca_train_loader, alpaca_val_loader, device)

    # 2) Finetune the model
    start_time = time.time()
    torch.manual_seed(123)

    optimizer = torch.optim.AdamW(pretrained_gpt_m.parameters(), lr=5e-5, weight_decay=0.1)
    num_epochs = 1
    total_steps = len(alpaca_train_loader) * num_epochs
    warmup_steps = int(0.01 * total_steps) # 20% warmup

    ift_alpaca_training_config = TrainingConfig(
        model=pretrained_gpt_m,
        train_loader=alpaca_train_loader,
        val_loader=alpaca_val_loader,
        optimizer=optimizer,
        device=device,
        num_epochs=num_epochs,
        eval_freq=25,
        eval_iter=5,
        start_context=format_input(alpaca_val_data[7]),
        tokenizer=tokenizer,
        warmup_steps=warmup_steps,
        initial_lr=1e-5,
        min_lr=1e-5,
        run_name=run_name,
    )
    # ift_alpaca_training_results = train_model_simple(ift_alpaca_training_config)
    ift_alpaca_training_results = train_model(ift_alpaca_training_config)

    end_time = time.time()
    execution_time_minutes = (end_time - start_time) / 60
    print(f"Training completed in {execution_time_minutes:.2f} minutes.")

    # 3) Saving and logging
    directory_path = f"{GOOGLE_DRIVE_PATH}/instr_fine_tune/data/runs/{run_name}/"
    if not os.path.exists(directory_path):
        os.makedirs(directory_path)

    # 3a) Save GPT model
    # model_file_path = directory_path + "pretrained_gpt_m.pth"
    # torch.save(pretrained_gpt_m.state_dict(), model_file_path)

    # 3b) Save training results
    training_results_file_path = directory_path + "training_results.json"
    save_training_results(ift_alpaca_training_results, training_results_file_path)

    # 3c) Plot learning curves
    plot_losses(ift_alpaca_training_config, ift_alpaca_training_results, directory_path + "loss_curves.png")
    plot_perplexity(ift_alpaca_training_config, ift_alpaca_training_results, directory_path + "perplexity_curves.png")

    # 3d) Generate sample responses
    generated_data_file_path = directory_path + "generated_data.json"
    generated_data = generate_model_response(
        model=pretrained_gpt_m,
        test_data=subset_alpaca_test_data,
        file_path=generated_data_file_path
    )

    return generated_data, ift_alpaca_training_results, ift_alpaca_training_config

run_name = "run39"
generated_data, ift_alpaca_training_results, ift_alpaca_training_config = finetune_model(run_name)

directory_path = f"{GOOGLE_DRIVE_PATH}/instr_fine_tune/data/runs/{run_name}/"
generated_data_file_path = directory_path + "generated_data.json"
generated_data_file_path

generated_data_scores = generate_model_scores(generated_data, generated_data_file_path)

plot_and_save_learning_rate(ift_alpaca_training_results, filepath=directory_path + "learning_rate.png")

"""## Scratch Pad"""

import json

import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
import torch
from typing import List, Optional

def _plot_metric_vs_epochs_comparison(
    training_config_a: TrainingConfig,
    tokens_seen_a: List[float],
    train_data_a: List[float],
    val_data_a: List[float],
    training_config_b: TrainingConfig,
    tokens_seen_b: List[float],
    train_data_b: List[float],
    val_data_b: List[float],
    run_a_label: str,
    run_b_label: str,
    y_label: str,
    filepath: Optional[str] = None
):
    """
    Helper function to plot training and validation metrics against epochs and tokens seen for two runs.

    Args:
        training_config_a: Configuration object for run A.
        tokens_seen_a: List or Tensor of cumulative tokens seen at each evaluation point for run A.
        train_data_a: List or Tensor of training metric values for run A.
        val_data_a: List or Tensor of validation metric values for run A.
        training_config_b: Configuration object for run B.
        tokens_seen_b: List or Tensor of cumulative tokens seen at each evaluation point for run B.
        train_data_b: List or Tensor of training metric values for run B.
        val_data_b: List or Tensor of validation metric values for run B.
        run_a_label: Prefix label for run A data lines.
        run_b_label: Prefix label for run B data lines.
        y_label: Label for the primary y-axis.
        filepath: Optional path to save the figure.
    """
    # Basic data validation for run A
    if not train_data_a:
        print(f"Warning: No training data provided for metric '{y_label}' for run A. Skipping plot.")
        return
    if not val_data_a:
        print(f"Warning: No validation data provided for metric '{y_label}' for run A. Skipping plot.")
        return
    if len(train_data_a) != len(val_data_a):
        print(f"Warning: Run A: Training data ({len(train_data_a)} points) and validation data ({len(val_data_a)} points) for metric '{y_label}' have different lengths. Plotting might be misleading.")
    if len(train_data_a) != len(tokens_seen_a):
         print(f"Warning: Run A: Metric data ({len(train_data_a)} points) and tokens_seen ({len(tokens_seen_a)} points) for metric '{y_label}' have different lengths. Token axis might be misaligned.")

    # Basic data validation for run B
    if not train_data_b:
        print(f"Warning: No training data provided for metric '{y_label}' for run B. Skipping plot.")
        return
    if not val_data_b:
        print(f"Warning: No validation data provided for metric '{y_label}' for run B. Skipping plot.")
        return
    if len(train_data_b) != len(val_data_b):
        print(f"Warning: Run B: Training data ({len(train_data_b)} points) and validation data ({len(val_data_b)} points) for metric '{y_label}' have different lengths. Plotting might be misleading.")
    if len(train_data_b) != len(tokens_seen_b):
         print(f"Warning: Run B: Metric data ({len(train_data_b)} points) and tokens_seen ({len(tokens_seen_b)} points) for metric '{y_label}' have different lengths. Token axis might be misaligned.")


    fig, ax1 = plt.subplots(figsize=(7, 5)) # Slightly larger figure for two runs

    # --- Plotting for Run A ---
    num_data_points_a = len(train_data_a)
    # Ensure epochs_seen aligns with the *number* of data points recorded for run A
    epochs_seen_a = torch.linspace(0, training_config_a.num_epochs, num_data_points_a if num_data_points_a > 0 else 1) # Avoid division by zero
    ax1.plot(epochs_seen_a, train_data_a, label=f"{run_a_label} Training", color='tab:blue', linestyle='-')
    # Ensure validation data is plotted against the same epoch scale
    ax1.plot(epochs_seen_a[:len(val_data_a)], val_data_a, label=f"{run_a_label} Validation", color='tab:blue', linestyle='-.')

    # --- Plotting for Run B ---
    num_data_points_b = len(train_data_b)
    # Ensure epochs_seen aligns with the *number* of data points recorded for run B
    epochs_seen_b = torch.linspace(0, training_config_b.num_epochs, num_data_points_b if num_data_points_b > 0 else 1) # Avoid division by zero
    ax1.plot(epochs_seen_b, train_data_b, label=f"{run_b_label} Training", color='tab:orange', linestyle='-')
    # Ensure validation data is plotted against the same epoch scale
    ax1.plot(epochs_seen_b[:len(val_data_b)], val_data_b, label=f"{run_b_label} Validation", color='tab:orange', linestyle='-.')


    ax1.set_xlabel("Epochs")
    ax1.set_ylabel(y_label)
    ax1.xaxis.set_major_locator(MaxNLocator(integer=True)) # only show integer labels on x-axis

    # Combine legends from both runs (for the lines)
    # FIX: Corrected typo here
    handles, labels = ax1.get_legend_handles_labels()
    ax1.legend(handles, labels, loc="upper right")


    # --- Create a second x-axis for tokens seen (combining tokens from both runs) ---
    ax2 = ax1.twiny() # Create a second x-axis that shares the same y-axis

    # To properly align the token axis, we need to consider the range of tokens seen
    # across both runs. We can plot invisible lines for both to ensure the axis
    # encompasses the full range and aligns ticks appropriately.
    ax2.plot(tokens_seen_a, train_data_a, alpha=0) # Invisible plot for aligning ticks A
    ax2.plot(tokens_seen_b, train_data_b, alpha=0) # Invisible plot for aligning ticks B


    ax2.set_xlabel("Tokens seen")
    # Optional: Format token ticks if they get large (e.g., K, M)
    ax2.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e6:.1f}M' if x >= 1e6 else (f'{x/1e3:.0f}K' if x >= 1e3 else f'{x:.0f}')))


    # --- Highlight best validation points for both runs ---

    # Run A best
    if val_data_a: # Only attempt if validation data exists
        min_val_loss_a = min(val_data_a)
        min_val_idx_a = val_data_a.index(min_val_loss_a)
        # Ensure index is within bounds of epochs_seen_a
        if min_val_idx_a < len(epochs_seen_a):
            min_epoch_a = epochs_seen_a[min_val_idx_a]
            ax1.scatter(min_epoch_a, min_val_loss_a, color='red', s=50, zorder=5)
            annotation_text_a = f'{run_a_label} Min Val: {min_val_loss_a:.3f}'
            # Calculate text position based on the range of both val datasets
            # Need to handle cases where one list is empty but the other isn't for max/min
            all_val_data = (val_data_a if val_data_a else []) + (val_data_b if val_data_b else [])
            max_val = max(all_val_data) if all_val_data else 1 # Default if no data
            min_val = min(all_val_data) if all_val_data else 0 # Default if no data
            y_offset_a = (max_val - min_val) * 0.15 if max_val > min_val else 0.15
            ax1.annotate(annotation_text_a,
                         xy=(min_epoch_a, min_val_loss_a),
                         xytext=(min_epoch_a, min_val_loss_a + y_offset_a),
                         textcoords='data', ha='center', va='bottom',
                         arrowprops=dict(arrowstyle="->", color='red', connectionstyle="arc3,rad=.2"))
        else:
            print(f"Warning: Best validation index ({min_val_idx_a}) out of bounds for epochs_seen_a ({len(epochs_seen_a)}). Cannot annotate best point for run A.")


    # Run B best
    if val_data_b: # Only attempt if validation data exists
        min_val_loss_b = min(val_data_b)
        min_val_idx_b = val_data_b.index(min_val_loss_b)
         # Ensure index is within bounds of epochs_seen_b
        if min_val_idx_b < len(epochs_seen_b):
            min_epoch_b = epochs_seen_b[min_val_idx_b]
            ax1.scatter(min_epoch_b, min_val_loss_b, color='purple', s=50, zorder=5)
            annotation_text_b = f'{run_b_label} Min Val: {min_val_loss_b:.3f}'
             # Calculate text position based on the range of both val datasets
            all_val_data = (val_data_a if val_data_a else []) + (val_data_b if val_data_b else [])
            max_val = max(all_val_data) if all_val_data else 1
            min_val = min(all_val_data) if all_val_data else 0
            y_offset_b = (max_val - min_val) * 0.05 if max_val > min_val else 0.05
            ax1.annotate(annotation_text_b,
                         xy=(min_epoch_b, min_val_loss_b),
                         xytext=(min_epoch_b, min_val_loss_b + y_offset_b),
                         textcoords='data', ha='center', va='bottom',
                         arrowprops=dict(arrowstyle="->", color='purple', connectionstyle="arc3,rad=.2"))
        else:
             print(f"Warning: Best validation index ({min_val_idx_b}) out of bounds for epochs_seen_b ({len(epochs_seen_b)}). Cannot annotate best point for run B.")

    # Removed the second redundant legend update call

    plt.grid(True) # Add grid for better readability

    fig.tight_layout() # Adjust layout to make room
    if filepath:
        try:
            plt.savefig(filepath)
            print(f"Plot saved to {filepath}")
        except Exception as e:
            print(f"Error saving plot to {filepath}: {e}")
    plt.show()


def plot_losses_comparison(training_config_a: TrainingConfig, training_results_a: TrainingResults,
                training_config_b: TrainingConfig, training_results_b: TrainingResults,
                run_a_label: str, run_b_label: str, filepath = None):
    """
    Plots training and validation losses against epochs and tokens seen for two runs for comparison.
    """
    print(f"Plotting losses comparison between '{run_a_label}' and '{run_b_label}'...")
    _plot_metric_vs_epochs_comparison(
        training_config_a=training_config_a,
        tokens_seen_a=training_results_a.track_tokens_seen,
        train_data_a=training_results_a.train_losses,
        val_data_a=training_results_a.val_losses,
        training_config_b=training_config_b,
        tokens_seen_b=training_results_b.track_tokens_seen,
        train_data_b=training_results_b.train_losses,
        val_data_b=training_results_b.val_losses,
        run_a_label=run_a_label,
        run_b_label=run_b_label,
        y_label="Loss",
        filepath=filepath
    )

def load_training_results(filepath, epochs=1):
    defaut_training_config = TrainingConfig(
        model=None,
        train_loader=None,
        val_loader=None,
        optimizer=None,
        device=None,
        num_epochs=epochs,
        eval_freq=5,
        eval_iter=5,
        start_context=None,
        tokenizer=None,
        warmup_steps=None,
        initial_lr=None,
        min_lr=None,
        run_name=None,
    )
    with open(filepath, 'r') as f:
        data = json.load(f)
        return defaut_training_config, TrainingResults(**data)

def get_training_results(run_name):
    directory_path = f"{GOOGLE_DRIVE_PATH}/instr_fine_tune/data/runs/{run_name}/"
    training_results_path = directory_path + "training_results.json"
    return load_training_results(training_results_path)

training_config_2, training_results_2 = get_training_results("run34")
training_config_4, training_results_4 = get_training_results("run36")

plot_losses(training_config_2, training_results_2)

test_directory_path = f"{GOOGLE_DRIVE_PATH}/instr_fine_tune/data/runs/test/"

plot_losses_comparison(training_config_2, training_results_2, training_config_4, training_results_4,
                       "20% warmup", "1% warmup",
                       test_directory_path + "run34_vs_run36.png")







